{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/sauxpa/github/reinforcement_trading/gym-rl_trading\n",
      "Requirement already satisfied: gym in /home/sauxpa/anaconda3/lib/python3.7/site-packages (from gym-rl-trading==0.0.1) (0.15.6)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /home/sauxpa/anaconda3/lib/python3.7/site-packages (from gym->gym-rl-trading==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/sauxpa/anaconda3/lib/python3.7/site-packages (from gym->gym-rl-trading==0.0.1) (1.17.2)\n",
      "Requirement already satisfied: scipy in /home/sauxpa/anaconda3/lib/python3.7/site-packages (from gym->gym-rl-trading==0.0.1) (1.4.1)\n",
      "Requirement already satisfied: six in /home/sauxpa/anaconda3/lib/python3.7/site-packages (from gym->gym-rl-trading==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/sauxpa/anaconda3/lib/python3.7/site-packages (from gym->gym-rl-trading==0.0.1) (1.4.10)\n",
      "Requirement already satisfied: future in /home/sauxpa/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-rl-trading==0.0.1) (0.17.1)\n",
      "Installing collected packages: gym-rl-trading\n",
      "  Found existing installation: gym-rl-trading 0.0.1\n",
      "    Uninstalling gym-rl-trading-0.0.1:\n",
      "      Successfully uninstalled gym-rl-trading-0.0.1\n",
      "  Running setup.py develop for gym-rl-trading\n",
      "Successfully installed gym-rl-trading\n"
     ]
    }
   ],
   "source": [
    "!pip install -e gym-rl_trading/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_rl_trading\n",
    "\n",
    "# RLLib\n",
    "import ray\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = gym.make('rl_trading-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brownian returns, no trend, no profit-making strategy.\n",
    "# env0 = gym.make('rl_trading-v0')\n",
    "\n",
    "# Brownian returns, +5% trend, profit-making long strategy.\n",
    "# env1 = gym.make('rl_trading-v1')\n",
    "\n",
    "# Brownian returns carried by a cyclical trend.\n",
    "# env2 = gym.make('rl_trading-v2')\n",
    "\n",
    "# Fractional Brownian, with local trend-following patterns.\n",
    "# env3 = gym.make('rl_trading-v3')\n",
    "\n",
    "# Fractional Brownian, with local mean-reversion patterns.\n",
    "# env4 = gym.make('rl_trading-v4')\n",
    "\n",
    "# Deterministic baseline with local trend-following patterns\n",
    "# env5 = gym.make('rl_trading-v5')\n",
    "\n",
    "# Deterministic baseline with local mean-reversion patterns\n",
    "# env6 = gym.make('rl_trading-v6')\n",
    "\n",
    "# Fractional Brownian, with local trend-following patterns.\n",
    "# Observations are augmented vith the empirical autocorrelogram.\n",
    "# env7 = gym.make('rl_trading-v7')\n",
    "\n",
    "# Fractional Brownian, with local mean-reversion patterns.\n",
    "# Observations are augmented vith the empirical autocorrelogram.\n",
    "# env8 = gym.make('rl_trading-v8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-25 11:12:06,939\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-02-25 11:12:06,941\tINFO resource_spec.py:212 -- Starting Ray with 1.95 GiB memory available for workers and up to 1.0 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-02-25 11:12:07,349\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "2020-02-25 11:12:07,864\tINFO trainer.py:420 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-02-25 11:12:07,936\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-02-25 11:12:07,938\tWARNING deprecation.py:30 -- DeprecationWarning: `exploration_final_eps` has been deprecated. Use `exploration_config.final_epsilon` instead. This will raise an error in the future!\n",
      "/home/sauxpa/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\n",
      "2020-02-25 11:12:10,767\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-13-10\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 88.03948201014151\n",
      "episode_reward_mean: -13.565169046477678\n",
      "episode_reward_min: -85.13929061510805\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 5\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 1.0\n",
      "  grad_time_ms: 22.194\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 153.42550659179688\n",
      "      mean_q: 57.236083984375\n",
      "      mean_td_error: 0.07731756567955017\n",
      "      min_q: -22.777681350708008\n",
      "      model: {}\n",
      "  num_steps_sampled: 5000\n",
      "  num_steps_trained: 256000\n",
      "  num_target_updates: 9\n",
      "  opt_peak_throughput: 11534.833\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 12.217\n",
      "  sample_time_ms: 4.688\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.91084337349398\n",
      "  ram_util_percent: 82.02048192771085\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.6119516033621889\n",
      "  mean_inference_ms: 0.8495179588044793\n",
      "  mean_processing_ms: 2.9308232420135845\n",
      "time_since_restore: 60.01312065124512\n",
      "time_this_iter_s: 60.01312065124512\n",
      "time_total_s: 60.01312065124512\n",
      "timestamp: 1582625590\n",
      "timesteps_since_restore: 5000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 5000\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-14-20\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 117.8986704568353\n",
      "episode_reward_mean: 17.81968638820201\n",
      "episode_reward_min: -85.13929061510805\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 10\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 1.0\n",
      "  grad_time_ms: 23.976\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 188.29212951660156\n",
      "      mean_q: 118.58406066894531\n",
      "      mean_td_error: 0.4930081069469452\n",
      "      min_q: 46.704368591308594\n",
      "      model: {}\n",
      "  num_steps_sampled: 10000\n",
      "  num_steps_trained: 576000\n",
      "  num_target_updates: 19\n",
      "  opt_peak_throughput: 10677.454\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 13.305\n",
      "  sample_time_ms: 5.133\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.159375000000004\n",
      "  ram_util_percent: 82.62604166666667\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.4775145960591029\n",
      "  mean_inference_ms: 0.8568437473429871\n",
      "  mean_processing_ms: 2.956101854523266\n",
      "time_since_restore: 129.63439011573792\n",
      "time_this_iter_s: 69.6212694644928\n",
      "time_total_s: 129.63439011573792\n",
      "timestamp: 1582625660\n",
      "timesteps_since_restore: 10000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 10000\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-15-30\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 4.55665692662235\n",
      "episode_reward_min: -361.11809906790813\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 15\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.5049999952316284\n",
      "  grad_time_ms: 24.111\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 257.92779541015625\n",
      "      mean_q: 170.62281799316406\n",
      "      mean_td_error: 2.869764804840088\n",
      "      min_q: 60.06672668457031\n",
      "      model: {}\n",
      "  num_steps_sampled: 15000\n",
      "  num_steps_trained: 896000\n",
      "  num_target_updates: 29\n",
      "  opt_peak_throughput: 10617.515\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 13.071\n",
      "  sample_time_ms: 4.975\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.565625000000004\n",
      "  ram_util_percent: 78.78125\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.4030761995298865\n",
      "  mean_inference_ms: 0.8615888231894265\n",
      "  mean_processing_ms: 2.951063694370713\n",
      "time_since_restore: 199.26457142829895\n",
      "time_this_iter_s: 69.63018131256104\n",
      "time_total_s: 199.26457142829895\n",
      "timestamp: 1582625730\n",
      "timesteps_since_restore: 15000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 15000\n",
      "training_iteration: 3\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-16-48\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 79.03088645946553\n",
      "episode_reward_min: -361.11809906790813\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 21\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.009999999776482582\n",
      "  grad_time_ms: 25.762\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 416.226318359375\n",
      "      mean_q: 206.32754516601562\n",
      "      mean_td_error: -2.2196738719940186\n",
      "      min_q: 45.48202896118164\n",
      "      model: {}\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 1216000\n",
      "  num_target_updates: 39\n",
      "  opt_peak_throughput: 9937.269\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 13.963\n",
      "  sample_time_ms: 5.192\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 4\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.73551401869159\n",
      "  ram_util_percent: 79.76074766355143\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.34812504176428294\n",
      "  mean_inference_ms: 0.8687432036424528\n",
      "  mean_processing_ms: 3.014708634470672\n",
      "time_since_restore: 277.5869333744049\n",
      "time_this_iter_s: 78.32236194610596\n",
      "time_total_s: 277.5869333744049\n",
      "timestamp: 1582625808\n",
      "timesteps_since_restore: 20000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 20000\n",
      "training_iteration: 4\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-18-07\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 59.130527346704945\n",
      "episode_reward_min: -361.11809906790813\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 26\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.009999999776482582\n",
      "  grad_time_ms: 29.679\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 357.7157897949219\n",
      "      mean_q: 250.91915893554688\n",
      "      mean_td_error: 2.374419927597046\n",
      "      min_q: 94.76033020019531\n",
      "      model: {}\n",
      "  num_steps_sampled: 25000\n",
      "  num_steps_trained: 1536000\n",
      "  num_target_updates: 49\n",
      "  opt_peak_throughput: 8625.52\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 16.446\n",
      "  sample_time_ms: 6.22\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.056363636363635\n",
      "  ram_util_percent: 79.17636363636362\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.31682376007609797\n",
      "  mean_inference_ms: 0.8752906089500102\n",
      "  mean_processing_ms: 3.0587189791286926\n",
      "time_since_restore: 356.67360043525696\n",
      "time_this_iter_s: 79.08666706085205\n",
      "time_total_s: 356.67360043525696\n",
      "timestamp: 1582625887\n",
      "timesteps_since_restore: 25000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 25000\n",
      "training_iteration: 5\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-19-22\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 46.49126015147139\n",
      "episode_reward_min: -361.11809906790813\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 31\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.009999999776482582\n",
      "  grad_time_ms: 27.939\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 373.31390380859375\n",
      "      mean_q: 267.007080078125\n",
      "      mean_td_error: 0.702216625213623\n",
      "      min_q: 70.39904022216797\n",
      "      model: {}\n",
      "  num_steps_sampled: 30000\n",
      "  num_steps_trained: 1856000\n",
      "  num_target_updates: 59\n",
      "  opt_peak_throughput: 9162.816\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 14.726\n",
      "  sample_time_ms: 5.351\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 6\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.77403846153846\n",
      "  ram_util_percent: 78.30096153846154\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.29280228695318117\n",
      "  mean_inference_ms: 0.8809180608114816\n",
      "  mean_processing_ms: 3.085578339308921\n",
      "time_since_restore: 432.0653827190399\n",
      "time_this_iter_s: 75.39178228378296\n",
      "time_total_s: 432.0653827190399\n",
      "timestamp: 1582625962\n",
      "timesteps_since_restore: 30000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 30000\n",
      "training_iteration: 6\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-20-39\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 20.422502941497292\n",
      "episode_reward_min: -364.706485604202\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 36\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.009999999776482582\n",
      "  grad_time_ms: 28.367\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 396.55908203125\n",
      "      mean_q: 279.2918701171875\n",
      "      mean_td_error: -1.0822710990905762\n",
      "      min_q: 44.665611267089844\n",
      "      model: {}\n",
      "  num_steps_sampled: 35000\n",
      "  num_steps_trained: 2176000\n",
      "  num_target_updates: 69\n",
      "  opt_peak_throughput: 9024.466\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 15.47\n",
      "  sample_time_ms: 5.984\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 7\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.43177570093458\n",
      "  ram_util_percent: 78.62056074766356\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.27375381378484764\n",
      "  mean_inference_ms: 0.8857887754196602\n",
      "  mean_processing_ms: 3.1061003201382666\n",
      "time_since_restore: 509.07448959350586\n",
      "time_this_iter_s: 77.00910687446594\n",
      "time_total_s: 509.07448959350586\n",
      "timestamp: 1582626039\n",
      "timesteps_since_restore: 35000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 35000\n",
      "training_iteration: 7\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-22-00\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 12.79071915996399\n",
      "episode_reward_min: -388.62875744174806\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 42\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.009999999776482582\n",
      "  grad_time_ms: 30.895\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 449.1153564453125\n",
      "      mean_q: 290.06768798828125\n",
      "      mean_td_error: 4.1701483726501465\n",
      "      min_q: 61.717750549316406\n",
      "      model: {}\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 2496000\n",
      "  num_target_updates: 79\n",
      "  opt_peak_throughput: 8286.036\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 15.036\n",
      "  sample_time_ms: 5.9\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 8\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.11636363636362\n",
      "  ram_util_percent: 79.02636363636366\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.2555909982216389\n",
      "  mean_inference_ms: 0.8910572000796807\n",
      "  mean_processing_ms: 3.136410442828835\n",
      "time_since_restore: 589.7920398712158\n",
      "time_this_iter_s: 80.71755027770996\n",
      "time_total_s: 589.7920398712158\n",
      "timestamp: 1582626120\n",
      "timesteps_since_restore: 40000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 40000\n",
      "training_iteration: 8\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-23-20\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 23.890254051439154\n",
      "episode_reward_min: -388.62875744174806\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 47\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.009999999776482582\n",
      "  grad_time_ms: 33.903\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 385.760986328125\n",
      "      mean_q: 294.4534606933594\n",
      "      mean_td_error: 1.5739635229110718\n",
      "      min_q: 65.33577728271484\n",
      "      model: {}\n",
      "  num_steps_sampled: 45000\n",
      "  num_steps_trained: 2816000\n",
      "  num_target_updates: 89\n",
      "  opt_peak_throughput: 7551.023\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 15.593\n",
      "  sample_time_ms: 5.621\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 9\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.32702702702703\n",
      "  ram_util_percent: 79.80540540540544\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.2432550263477109\n",
      "  mean_inference_ms: 0.8949521925720829\n",
      "  mean_processing_ms: 3.1561767468628963\n",
      "time_since_restore: 669.7077088356018\n",
      "time_this_iter_s: 79.91566896438599\n",
      "time_total_s: 669.7077088356018\n",
      "timestamp: 1582626200\n",
      "timesteps_since_restore: 45000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 45000\n",
      "training_iteration: 9\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-02-25_11-24-45\n",
      "done: false\n",
      "episode_len_mean: 950.0\n",
      "episode_reward_max: 511.54447347455414\n",
      "episode_reward_mean: 24.49883610792171\n",
      "episode_reward_min: -388.62875744174806\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 52\n",
      "experiment_id: 2b5ac46d24254c369c67603e1d054979\n",
      "hostname: sauxpa\n",
      "info:\n",
      "  exploration_infos:\n",
      "  - 0.009999999776482582\n",
      "  grad_time_ms: 28.253\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_lr: 0.0020000000949949026\n",
      "      max_q: 383.9123840332031\n",
      "      mean_q: 298.5076904296875\n",
      "      mean_td_error: -0.598906934261322\n",
      "      min_q: -11.22369384765625\n",
      "      model: {}\n",
      "  num_steps_sampled: 50000\n",
      "  num_steps_trained: 3136000\n",
      "  num_target_updates: 99\n",
      "  opt_peak_throughput: 9061.082\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 16.156\n",
      "  sample_time_ms: 5.509\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.43.102\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.571304347826086\n",
      "  ram_util_percent: 81.44521739130435\n",
      "pid: 8099\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.2328049635283828\n",
      "  mean_inference_ms: 0.8991469337592428\n",
      "  mean_processing_ms: 3.1747594094552207\n",
      "time_since_restore: 754.1799538135529\n",
      "time_this_iter_s: 84.47224497795105\n",
      "time_total_s: 754.1799538135529\n",
      "timestamp: 1582626285\n",
      "timesteps_since_restore: 50000\n",
      "timesteps_this_iter: 5000\n",
      "timesteps_total: 50000\n",
      "training_iteration: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def train_dqn(n_iter):\n",
    "  config = dqn.DEFAULT_CONFIG.copy()\n",
    "  config[\"log_level\"] = \"WARN\"\n",
    "  config[\"lr\"] = 0.002\n",
    "  config[\"gamma\"] = 0.99\n",
    "  config[\"train_batch_size\"] = 256\n",
    "  config[\"buffer_size\"] = 10000\n",
    "  config[\"timesteps_per_iteration\"] = 5000\n",
    "  config[\"hiddens\"] = [64, 64]\n",
    "  config[\"exploration_final_eps\"] = 0.01\n",
    "  trainer = dqn.DQNTrainer(config=config, env='rl_trading-v4')\n",
    "  for i in range(n_iter):\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    \n",
    "train_dqn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.1.0 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "### to view training results, run in a terminal:\n",
    "!tensorboard --logdir=~/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
